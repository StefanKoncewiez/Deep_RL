{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cce9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5471610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True        \n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c909771",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55adec8a",
   "metadata": {},
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25aa4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and use multiprocessing (or not) to run multiple env in parallel when creating episodes for a policy\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "    \n",
    "num_envs = 16\n",
    "env_name = \"MountainCarContinuous-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        env._max_episode_steps = 1000\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = DummyVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa1cbd",
   "metadata": {},
   "source": [
    "### Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(131)    # 1 row and 3 column, chose the first case\n",
    "    plt.title(f\"frame {frame_idx}. reward: {rewards[-1]}\")\n",
    "    plt.plot(rewards)   # use the index as x\n",
    "    plt.show()\n",
    "\n",
    "def test_env(vis=False):\n",
    "    # The single environment is a raw gymnasium env -> .reset() returns 2 values in a tuple\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    if vis:\n",
    "        env.render()\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = (torch\n",
    "                        .from_numpy(state)\n",
    "                        .unsqueeze(0)\n",
    "                        .to(device))\n",
    "        \n",
    "        dist, _ = model(state_tensor)   # the critic part of the model also return the sate value estimation\n",
    "\n",
    "        # The single environment is a raw gymnasium env -> return 5 values in a tuple\n",
    "        next_state, reward, terminated, truncated, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if vis:\n",
    "            env.render()\n",
    "\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7d229",
   "metadata": {},
   "source": [
    "### GAE (Generalized Advatage Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce131b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.9999, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc5df6",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bafd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)                      # using current policy and critic network\n",
    "            entropy = dist.entropy().mean()                 # compute entropy to force exploration in the loss function\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()  # we take the mean over the minibatch to approximate the expected value of the surrogate loss\n",
    "            critic_loss = (return_ - value).pow(2).mean()   # MSE ovet the minibatch\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945fcb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 2048\n",
    "mini_batch_size  = 64\n",
    "ppo_epochs       = 10\n",
    "threshold_reward = 100\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 200000\n",
    "frame_idx = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state = envs.reset()\n",
    "early_stop = False\n",
    "total_steps = 0\n",
    "csv_dict = {\"num_step\": [], \"avg_reward\": []}\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        # ====== gather the data for the current policy ======\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 1000 == 0:\n",
    "            test_reward = np.mean([test_env() for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            # save for the csv\n",
    "            csv_dict[\"num_step\"].append(total_steps)\n",
    "            csv_dict[\"avg_reward\"].append(test_reward)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "        #----------------------------------------------------\n",
    "            \n",
    "\n",
    "    # ====== Prepare the data for the PPO update and run the update ====== \n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
    "\n",
    "# save the csv file\n",
    "csv_df = pd.DataFrame.from_dict(csv_dict)\n",
    "csv_df.to_csv(\"ppo_training_rewards_mc_c.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbe5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rewards = [test_env() for _ in range(10)]\n",
    "print(test_rewards.mean())\n",
    "print(test_rewards.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fba26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained modelâ€™s parameters\n",
    "answer = input(\"Are you sure you want to save the current model ?\")\n",
    "save_path = f\"{env_name}.pth\"\n",
    "\n",
    "if answer==\"yes\":\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model parameters saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ab714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh instance of the network and load the weights\n",
    "loaded_model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "loaded_model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "loaded_model.eval()\n",
    "print(\"Model loaded and set to eval mode\")\n",
    "\n",
    "# 3) Run one episode with rendering\n",
    "render_env = gym.make(env_name, render_mode=\"human\")\n",
    "state, _ = render_env.reset()\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "\n",
    "while not done:\n",
    "    render_env.render()\n",
    "    # prepare state tensor\n",
    "    state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        dist, _ = loaded_model(state_tensor)\n",
    "    action = dist.mean.cpu().numpy()[0]  # deterministic for rendering\n",
    "    \n",
    "    # step the env\n",
    "    next_state, reward, terminated, truncated, _ = render_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "print(f\"Episode finished, total reward: {total_reward:.2f}\")\n",
    "render_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f1f5c",
   "metadata": {},
   "source": [
    "### Record a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# 1) Build a timestamp string\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 2) Wrap your environment, embedding the timestamp in the video name\n",
    "video_env = RecordVideo(\n",
    "    gym.make(env_name, render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos\",\n",
    "    # This prefix (plus episode number) will ensure uniqueness\n",
    "    name_prefix=f\"{env_name}-{timestamp}\",\n",
    "    episode_trigger=lambda idx: True\n",
    ")\n",
    "\n",
    "# 3) Run one episode (no inline render calls)\n",
    "state, _ = video_env.reset()\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        dist, _ = loaded_model(state_tensor)\n",
    "    action = dist.mean.cpu().numpy()[0]\n",
    "\n",
    "    next_state, reward, terminated, truncated, _ = video_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "video_env.close()\n",
    "print(f\"Recorded episode, total reward = {total_reward:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
